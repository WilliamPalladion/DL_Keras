{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Four branches of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Supervised learning: learn how to map input data to known targets (*annotations*), given a set of examples\n",
    "\n",
    "### 2) Unsupervised learning: find transformations of the input data without the help of any targets, for the purpose of data visualization, compression, denoising or better understanding of the correlations (e.g. *Dimensionality reduction* and *Clustering*)\n",
    "\n",
    "### 3) Self-supervised learning: No human-annotated labels, generated from the input data, typically using a heuristic algorithm\n",
    "\n",
    "### 4) Reinforcement learning: an *agent* receives information about its environment and learns to choose actions that will maximize the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating machine-learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In ML, the goal is to achieve models that *generalize* --- perform well on never-before-seen data --- and *overfitting* is the central obstacle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following sections look at strategies for mitigating overfitting and maximizing generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Training, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluating a model always boils down to splitting the available data into 3 sets: training, validation and test\n",
    "\n",
    "###  Why not have 2 two sets: training & test?   ----  Because developing a model always involves *tuning its configuration* (e.g. choose the number of layers or size of layers --- *hyper-parameters*)\n",
    "###  You do this tuning by seeing the feedback signal from the performance on validation data (in essence, tuning is a form of *learning*: a search for a good configuration in some parameter space). As a result, tuning based on the validation set will soon cause *overfitting to the validation set*\n",
    "\n",
    "###  Centeral to this phenomenon is the notion of *information leaks*: each time you tune a hyper-parameter of the model based on the performance on validation set, some information about the validation set *leaks* into the model, the validation set becomes less reliable to evaluate the model.\n",
    "\n",
    "### Thus our model should not have had access to *any* information about the test set,  even indirectly. That' s why we set a validation set separately rather than 2 sets simply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Besides this splitting, we still have several advanced ways to do that come in handy when little data is available: 1) simple hold-out validation; 2) K-fold validaiton; 3) iterated K-fold validation with shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple hold-out validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set apart some fraction of data as the test set. Train on the remaining data, and evaluate on the test data. (Of, course, we should reserve a validation set to avoid *information leaks*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold-out validation\n",
    "\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data)  # shuffling data is usually appropriate\n",
    "\n",
    "# split the validation set\n",
    "validation_data = data[: num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "# split the training data\n",
    "training_data = data[:]\n",
    "\n",
    "# train a model on training data and evaluate it on the validation data\n",
    "model = get_model()\n",
    "model.train(traning_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "'''\n",
    "At this point, you can tune your model, retrain it, evaluate it, tune it again,....etc.\n",
    "to find the best hyper-parameters\n",
    "'''\n",
    "\n",
    "# once you've tuned your hyper-parameters, it's common to train your final model from scratch\n",
    "# on all non-test data available\n",
    "\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### it suffers from one flaw: if little data is available, then the validation & test set may contain too few samples to be statistically representative of the data at hand;\n",
    "### This problem is easy to recognize: if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance, then you are facing this problem\n",
    "\n",
    "### K-fold validation and iterated K-fold validation are two ways to address this problem, as discussed next:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def: split data into K partitions of equal size. For each partition i, train a model on the remaining K-1 partitions and evaluate it on partition i. The final score is then the average of the K scores obtained.\n",
    "\n",
    "### This method is helpful when the performance of your model shows siginificant variance based on your training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for i in range(k):\n",
    "    validation_data = data[i * num_validation_samples : (i+1) * num_validation_samples]\n",
    "    training_data = data[:num_validation_samples * i] + data[num_validation_samples * (i+1):]\n",
    "    # note '+' here is list concatenation operator,not summation\n",
    "    \n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "average_validation_score = np.average(validation_scores)\n",
    "\n",
    "# tune the model and train the final model\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated K-fold validation with shuffling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This approach is for situations in which you have ralatively little data available and you need to evaluate the model as precisely as possible;\n",
    "### It consists of applying K-fold validation multiple times, shuffling the data every time before spliting it K ways. The final score is the average of th scores obtained at each run of K-fold validation.\n",
    "### Note that you end up training and evaluate P &times; K models (P is the number of iterations you use), which can be very expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Things to keep in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep an eye out for the following when choosing an evaluation protocal:\n",
    "\n",
    "### 1) Data representativeness: \n",
    "\n",
    "### we hope both training set and test set to be representative of the data at hand. e.g. classify images of digits, you start from samples ordered by class 0~9, and directly take 80% as training set and the remaining 20% as test set, this will result a ridiculus mistake: your training set only contains class 0~7, while test set only class 8~9. Thus, we ususally should *randomly shuffle* the data before splitting\n",
    "\n",
    "### 2) The arrow of time:  \n",
    "### if we try to predict the future given the past, we should *not* shuffle data because doing this will create a *temporal leak*: the model will be effectively trained on the *future* data. You should make sure all data in test set is *posterior* to data in the training set\n",
    "\n",
    "### 3) Redundancy in data: \n",
    "### if some data points appear twice or even more, then shuffing it into training & test set will result in redundancy, in effect, we are testing on part of the training data. Thus we should make sure the training set and validation set are *disjoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data preprocessing, feature engineering and feature learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though many data-preprocessing and feature-engineering techniques are *domain specific*, we first review the basics that are common to all data domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 Data preprocessing for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing aims at making the raw data at hand more amenable to neural networks.\n",
    "### This includes 1) vectorization; 2) normalization; 3) handling missing values; 4) feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All inputs and targets in a neural network must be *tensors of floating-point data* (or in specific cases, tensors of integers). Whatever data to process, we must first turn it into tensors, a step called *data vectorization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Value normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, it is not safe to feed into a neural network data that 1) takes relatively large values 2) heterogeneous scale. Doing so will trigger large gradient updates that will prevent the network from converging.\n",
    "\n",
    "### To make learning more easier for the network, the data should have following characteristics:\n",
    "### 1) Take  small values --- typically most values should be in 0~1 range;\n",
    "### 2) Be homogeneous --- all features should take values in roughly the same range;\n",
    "\n",
    "### Additionally, we might have stricter normalization rules, although it is not always necessary:\n",
    "### ---> Normalize each feature independently to have a mean of 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily to do with Numpy arrays\n",
    "\n",
    "x -= x.mean(axis = 0)\n",
    "x /= x.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, with neural networks, it's safe to input missing values as 0, with the condition that 0 is not already a meaningful value.\n",
    "### The network will learn from exposure to the data that the value 0 means *missing data* and will start to *learn* to ignore the value.\n",
    "\n",
    "### Note that if you expect misssing values in test data, but the network was trained on data without any missing values, the network won't have learned to ignore missing values !!\n",
    "\n",
    "### In this situation, we should artificially generate training samples with missing values: e.g. copy some training samples several times and drop some of the features that you expect are likely to be missing in the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature engineering* is the process of using your own knowledge about the data and the ML algorithm at hand to make the algorithm work better by applying hardcoded (non-learned) transformations to the data before it goes into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In many cases, it isn't resonable to expect a ML model to be able to learn from completely arbitrary data. The data needs to be presented in a way that will make the model's job easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The essence of feature engineering: making a problem easier by expressing it in a simpler way. It usually requires understanding the problem in depth.\n",
    "\n",
    "### Fortunately, modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data\n",
    "\n",
    "### But good feture engineeing still helps !  For 2 reasons:\n",
    "### 1) allow us to solve problems more elegantly while using fewer resources;\n",
    "### 2) let you solve a problem with far less data: if we only have a few samples, then the information value in their features becomes critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fundamental issue in ML is the tension between *optimization* & *generalization*:\n",
    "\n",
    "### *Optimization* refers to the process of adjusting a model to get the best performance possible on training data (*learning* in *machine learning*);\n",
    "### 'Generalization' refers to how well the trained model performs on data it has never seen before;\n",
    "### Though the goal is to get good generalizaiton, but we cannot control generalization, we can only adjust the model based on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the begining of the training, optimization and generalization are correlated: the lower the loss on training date, the lower on the loss on test data ----------While this happens, the model is said to be *underfit*: there is still progress to be made; The network has not yet modeled all relevant patterns in the training data.\n",
    "\n",
    "### But after a certain number of iterations on training data, generalization stops improving and validation metrics stall and begin to degrade  ----------The model is starting to *overfit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To prevent overfitting (learning misleading and irrelevant patterns in traning data):\n",
    "### 1) Best solution is to *get more training data*;\n",
    "### 2) When it's not possible, the next-best solution is to modulate the quantity of information that your model is allowed to store or to add constraints on what information it's allowed to store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process of fighting overfitting this way is called *Regularization*. Let's see some most common regularization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 Reducing the network's size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the size of the model: the number of learnable parameters in the model (num of layers &  units per layer)  -------- which is so called the model's *capacity*. \n",
    "\n",
    "### The model with more parameters has more *memorization capacity* and therefore can easily learn a perfect dictionary-like mapping (without any generalization power), but such a model is useless for predicting new things\n",
    "\n",
    "### On the other hand, if the network has limited memorization resources, it won't be able to learn this mapping as easily; thus in order to minimize the loss, it will have to resort to learning compressed representations that have predictable power regarding the targets ---- precisely the type of representations we're interested in (BUT you need to make sure you model don't *underfit* !!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortuately, no magical formula to determine the right number of capacity used in network, we must evaluate an array of different architectures (on validation set, of course)\n",
    "### The general workflow is to start with relatively few layers and parameters, and increase the size of layers or add new layers until you see diminishing returns with regard to validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 Adding weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occam's razor (奥卡姆剃刀原理): Given two explanations for something, the explanation most likely to be correct is the simplest one --- the one with fewer assumptions;   ----- simple models are less likely to overfit than complex ones\n",
    "\n",
    "###  A *simple* model here is a model where the distribution of parameter values has *less entropy* (fewer parameters). \n",
    "\n",
    "### Thus a common way to mitigate the overfitting is to put constraints on teh complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more *regular* ------This is so-called *weight regularization*\n",
    "\n",
    "### *weight regularization* is done by adding to the loss function of the network a *cost* associated with having large weights. Generally the cost comes in 2 flavors:\n",
    "\n",
    "### 1) *L1 regularization*  ---- cost is proportional to the *abouslute value of weight coefficients* (L1 norm);\n",
    "### 2) *L2 regularizaiton*  ---- cost is proportional to the *square of the value of the weight coefficients* (L2 norm)\n",
    "### **L2 regularization is also called *weight decay* in the context of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Keras, weight regularization is added by passing *weight regularizer instances* to layers as keyword arguments. Let's see the example of movie-review classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add L2 regularization to the model\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer = regularizers.l2(0.001),\n",
    "                       activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, kernel_regularizer = regularizers.l2(0.001),\n",
    "                       activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l2(0.001) means every coefficient in the weight matrix of the layer wiil add 0.001 &times; weight_coefficient_value to the total loss of the network. Note that because this penalty is *only added at training time*,  the loss for the network will be much higher at training than at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We may have other choices for weight regularizers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "regularizers.l1(0.001)  # L1 regullarization\n",
    "\n",
    "regularizers.l1_l2(l1 = 0.001, l2 = 0.001)  # simultaneous L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.3 Adding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropout*, applied to a layerm consists of randomly *dropping out* (setting to 0) a number of output features of the layer during training. \n",
    "\n",
    "### e.g. [0.2, 0.5, 1.3, 0.8, 1.1] is the output of a given layer, after dropping out, this vector will have a few zero entries distributed ramdomly: e.g. [0, 0.5, 1.3, 0, 1.1]\n",
    "\n",
    "### *dropout rate*: the fraction of the features that are zeroed out (usually between 0.2~0.5)\n",
    "### At test time, no units are dropped out, instead the layer's output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at traning time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.array([3,  1, 10, 15])\n",
    "\n",
    "x = np.random.randint(0, high=2, size = test.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider a numpy matrix containing the output of a layer,  *layer_output*, of shape (batch_size, features). At training time, we zero out at ramdom a fraction of the values in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output *= np.random.randint(0, high = 2, size = layer_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At test time, we scale down the output by the dropout rate. Here we scale by 0.5 for instance if previously dropped half of the units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is often the way implemented in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output *= np.random.randint(0, high = 2, size = layer_output.shape)\n",
    "layer_output /= 0.5\n",
    "# Note here we scaling up rather than scaling down in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The core idea is to introduce noise in the output values of a layer can break up happenstance patterns that are not significant, which the network will start memorizing if no noise is present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Keras, we introduce dropout in a network via the *Dropout* layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB example: we add 2 Dropout layers \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(16, activation = 'relu',))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To recap, the most common ways to prevent overfitting:\n",
    "## 1) Get more training data;\n",
    "## 2) Reduce the capacity of the network;\n",
    "## 3) Add weight regularization;\n",
    "## 4) Add dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
